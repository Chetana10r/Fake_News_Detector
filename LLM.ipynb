{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai pandas scikit-learn flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\git repos\\venv\\lib\\site-packages (1.65.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\git repos\\venv\\lib\\site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\git repos\\venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\git repos\\venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\git repos\\venv\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\git repos\\venv\\lib\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\git repos\\venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\git repos\\venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\git repos\\venv\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\git repos\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\git repos\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\git repos\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\git repos\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\git repos\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\git repos\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: colorama in c:\\git repos\\venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  Ben Stein Calls Out 9th Circuit Court: Committ...   \n",
      "1  Trump drops Steve Bannon from National Securit...   \n",
      "2  Puerto Rico expects U.S. to lift Jones Act shi...   \n",
      "3   OOPS: Trump Just Accidentally Confirmed He Le...   \n",
      "4  Donald Trump heads for Scotland to reopen a go...   \n",
      "\n",
      "                                                text       subject  \\\n",
      "0  21st Century Wire says Ben Stein, reputable pr...       US_News   \n",
      "1  WASHINGTON (Reuters) - U.S. President Donald T...  politicsNews   \n",
      "2  (Reuters) - Puerto Rico Governor Ricardo Rosse...  politicsNews   \n",
      "3  On Monday, Donald Trump once again embarrassed...          News   \n",
      "4  GLASGOW, Scotland (Reuters) - Most U.S. presid...  politicsNews   \n",
      "\n",
      "                  date  label  \n",
      "0    February 13, 2017      0  \n",
      "1       April 5, 2017       1  \n",
      "2  September 27, 2017       1  \n",
      "3         May 22, 2017      0  \n",
      "4       June 24, 2016       1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load fake news data\n",
    "df_fake = pd.read_csv(\"Fake.csv\")\n",
    "df_fake[\"label\"] = 0  # 0 means Fake\n",
    "\n",
    "# Load real news data\n",
    "df_real = pd.read_csv(\"True.csv\")\n",
    "df_real[\"label\"] = 1  # 1 means Real\n",
    "\n",
    "# Combine both datasets\n",
    "df = pd.concat([df_fake, df_real], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Shuffle the dataset\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Drop duplicate values (if any)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'text', 'subject', 'date', 'label'], dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cheta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cheta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
      "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
      "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
      "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
      "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
      "\n",
      "                                                text subject  \\\n",
      "0  Donald Trump just couldn t wish all Americans ...    News   \n",
      "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
      "2  On Friday, it was revealed that former Milwauk...    News   \n",
      "3  On Christmas day, Donald Trump announced that ...    News   \n",
      "4  Pope Francis used his annual Christmas Day mes...    News   \n",
      "\n",
      "                date  label  \n",
      "0  December 31, 2017      0  \n",
      "1  December 31, 2017      0  \n",
      "2  December 30, 2017      0  \n",
      "3  December 29, 2017      0  \n",
      "4  December 25, 2017      0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the dataset\n",
    "df_fake = pd.read_csv(\"Fake.csv\")\n",
    "df_real = pd.read_csv(\"True.csv\")\n",
    "\n",
    "# Label the data\n",
    "df_fake[\"label\"] = 0  # Fake News\n",
    "df_real[\"label\"] = 1  # Real News\n",
    "\n",
    "# Combine both datasets\n",
    "df = pd.concat([df_fake, df_real], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Text Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    words = word_tokenize(text)  # Tokenize text\n",
    "    words = [word for word in words if word not in stop_words and len(word) > 2]  # Remove stopwords\n",
    "    return ' '.join(words)\n",
    "\n",
    "#df['text'] = df['text'].apply(clean_text)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/import openai\\nimport os\\n\\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\\n\\ntry:\\n   models = openai.models.list()\\n   print(models)\\nexcept openai.error.AuthenticationError:\\n    print(\"Invalid API Key!\")'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''/import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "try:\n",
    "   models = openai.models.list()\n",
    "   print(models)\n",
    "except openai.error.AuthenticationError:\n",
    "    print(\"Invalid API Key!\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import openai\\n\\ndef get_embedding(text):\\n    response = openai.embeddings.create(\\n        model=\"text-embedding-3-small\",\\n        input=text\\n    )\\n    return response.data[0].embedding  # Extract the embedding\\n\\n# Example usage\\ntext = \"AI is transforming the world.\"\\nembedding = get_embedding(text)\\nprint(embedding)  # Prints the vector representation of the text\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import openai\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = openai.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding  # Extract the embedding\n",
    "\n",
    "# Example usage\n",
    "text = \"AI is transforming the world.\"\n",
    "embedding = get_embedding(text)\n",
    "print(embedding)  # Prints the vector representation of the text\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.model_selection import train_test_split\\n\\n# Convert embeddings to NumPy arrays\\nX = np.vstack(df[\"embeddings\"].values)\\ny = df[\"label\"].values\\n\\n# Split Data into Training and Testing Sets\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\nprint(\"Data Split Successfully!\")\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert embeddings to NumPy arrays\n",
    "X = np.vstack(df[\"embeddings\"].values)\n",
    "y = df[\"label\"].values\n",
    "\n",
    "# Split Data into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Data Split Successfully!\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import accuracy_score, classification_report\\n\\n# Train Model\\nmodel = LogisticRegression()\\nmodel.fit(X_train, y_train)\\n\\n# Save Model\\nimport pickle\\npickle.dump(model, open(\"fake_news_model.pkl\", \"wb\"))\\n\\n# Model Evaluation\\ny_pred = model.predict(X_test)\\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\\nprint(classification_report(y_test, y_pred))'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train Model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save Model\n",
    "import pickle\n",
    "pickle.dump(model, open(\"fake_news_model.pkl\", \"wb\"))\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred))'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from flask import Flask, request, jsonify\\n\\napp = Flask(__name__)\\n\\n# Load Model\\nmodel = pickle.load(open(\"fake_news_model.pkl\", \"rb\"))\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    data = request.json[\"text\"]\\n    embedding = get_embedding(data).reshape(1, -1)\\n    prediction = model.predict(embedding)[0]\\n    result = \"Real News\" if prediction == 1 else \"Fake News\"\\n    return jsonify({\"prediction\": result})\\n\\nif __name__ == \\'__main__\\':\\n    app.run(debug=True)'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load Model\n",
    "model = pickle.load(open(\"fake_news_model.pkl\", \"rb\"))\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json[\"text\"]\n",
    "    embedding = get_embedding(data).reshape(1, -1)\n",
    "    prediction = model.predict(embedding)[0]\n",
    "    result = \"Real News\" if prediction == 1 else \"Fake News\"\n",
    "    return jsonify({\"prediction\": result})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
